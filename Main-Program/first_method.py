# -*- coding: utf-8 -*-
"""First_method.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X_-Ks3sFWskTZTtz54Lud-l_JiAa1loT
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install opencv-contrib-python
!pip install torch
!pip install torchvision
!pip install imutils
!pip install matplotlib
!pip install numpy
!pip install tqdm

import matplotlib.pyplot as plt
from torchvision.io import read_image


image = read_image("/content/drive/MyDrive/Dataset/renders/train/particles5.png")
mask = read_image("/content/drive/MyDrive/Dataset/renders/train/particles5.png")

plt.figure(figsize=(16, 8))
plt.subplot(121)
plt.title("Image")
plt.imshow(image.permute(1, 2, 0))
plt.subplot(122)
plt.title("Mask")
plt.imshow(mask.permute(1, 2, 0))

import os
import torch

from torchvision.io import read_image
from torchvision.ops.boxes import masks_to_boxes
from torchvision import tv_tensors
from torchvision.transforms.v2 import functional as F

import torchvision
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor

# load a model pre-trained on COCO
model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)

# replace the classifier with a new one, that has
# num_classes which is user-defined
num_classes = 2  # 1 class (person) + background
# get number of input features for the classifier
in_features = model.roi_heads.box_predictor.cls_score.in_features
# replace the pre-trained head with a new one
model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)

import torchvision
from torchvision.models.detection import FasterRCNN
from torchvision.models.detection.rpn import AnchorGenerator

# load a pre-trained model for classification and return
# only the features
backbone = torchvision.models.mobilenet_v2(weights="DEFAULT").features
# ``FasterRCNN`` needs to know the number of
# output channels in a backbone. For mobilenet_v2, it's 1280
# so we need to add it here
backbone.out_channels = 1280

# let's make the RPN generate 5 x 3 anchors per spatial
# location, with 5 different sizes and 3 different aspect
# ratios. We have a Tuple[Tuple[int]] because each feature
# map could potentially have different sizes and
# aspect ratios
anchor_generator = AnchorGenerator(
    sizes=((32, 64, 128, 256, 512),),
    aspect_ratios=((0.5, 1.0, 2.0),)
)

# let's define what are the feature maps that we will
# use to perform the region of interest cropping, as well as
# the size of the crop after rescaling.
# if your backbone returns a Tensor, featmap_names is expected to
# be [0]. More generally, the backbone should return an
# ``OrderedDict[Tensor]``, and in ``featmap_names`` you can choose which
# feature maps to use.
roi_pooler = torchvision.ops.MultiScaleRoIAlign(
    featmap_names=['0'],
    output_size=7,
    sampling_ratio=2
)

# put the pieces together inside a Faster-RCNN model
model = FasterRCNN(
    backbone,
    num_classes=2,
    rpn_anchor_generator=anchor_generator,
    box_roi_pool=roi_pooler
)

import torchvision
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor


def get_model_instance_segmentation(num_classes):
    # load an instance segmentation model pre-trained on COCO
    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights="DEFAULT")

    # get number of input features for the classifier
    in_features = model.roi_heads.box_predictor.cls_score.in_features
    # replace the pre-trained head with a new one
    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)

    # now get the number of input features for the mask classifier
    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels
    hidden_layer = 256
    # and replace the mask predictor with a new one
    model.roi_heads.mask_predictor = MaskRCNNPredictor(
        in_features_mask,
        hidden_layer,
        num_classes
    )

    return model

!pip install bpy

import os
import torch

from torchvision.io import read_image, ImageReadMode
from torchvision.ops.boxes import masks_to_boxes
from torchvision import tv_tensors
from torchvision.transforms.v2 import functional as F
import json
import math
import numpy as np


class PennFudanDataset(torch.utils.data.Dataset):
    def __init__(self, root, transforms):
        self.root = root
        self.transforms = transforms
        # load all image files, sorting them to
        # ensure that they are aligned
        #self.imgs = list(sorted(os.listdir(os.path.join(root, "PNGImages"))))
        #self.masks = list(sorted(os.listdir(os.path.join(root, "PedMasks"))))
        self.imgs = list(sorted(os.listdir(os.path.join("/content/drive/MyDrive/Dataset/renders/", root))))
        self.masks = list(sorted(os.listdir(os.path.join("/content/drive/MyDrive/Dataset/particles/", root))))
        #self.imgs = list(sorted(os.listdir(os.path.join(root, "train"))))
        #self.masks = list(sorted(os.listdir(os.path.join(root, "json"))))

    def __getitem__(self, idx):
        # load images and masks
          img_path = os.path.join("/content/drive/MyDrive/Dataset/renders/", self.root, self.imgs[idx])
          mask_path = os.path.join("/content/drive/MyDrive/Dataset/renders/", self.root, self.imgs[idx])
          # img_path = os.path.join(self.root, "train", self.imgs[idx])
          # mask_path = os.path.join(self.root, "train", self.imgs[idx])
          img = read_image(img_path, mode=ImageReadMode.RGB).float()
          #mask = read_image(mask_path)
          annotations_file = open(os.path.join("/content/drive/MyDrive/Dataset/particles/", self.root, self.masks[idx]))
          #annotations_file = open(os.path.join(self.root, "json", self.masks[idx]))
          annotations = json.load(annotations_file)
          img_filename = annotations['distribution']
          points = annotations['particles']

          size = []
          tn = []
          scale = np.array([11.0, 11.0, 2.0])
          import bpy
          import bpy_extras
          from mathutils import Vector

          i=6
          for el in points:
            size.append(el['size'])
            x = (scale[0] * (el['x']) - scale[0]/2)
            y = (scale[1] * (el['y']) - scale[1]/2)
            z = (scale[2] * (el['z']) - scale[2]/2)
            if i == 6:
              scene = bpy.data.scenes["Scene"]

              scene.render.resolution_x = 1024
              scene.render.resolution_y = 1024

              scene.camera.rotation_mode = 'XYZ'
              scene.camera.rotation_euler = (0.0, 0.0, 0.0)

              scene.camera.location.x = 0
              scene.camera.location.y = 0
              scene.camera.location.z = 10

              bpy.context.scene.render.engine = 'CYCLES'
              bpy.ops.mesh.primitive_uv_sphere_add(
                  radius=points[i]['size'],
                  location=(x,y,z))
              i += 1
            coord_3d = Vector((x, y, z))
            coords_2d = bpy_extras.object_utils.world_to_camera_view(scene, scene.camera, coord_3d)


            x = (coords_2d.x) * (1024)
            y = (1 - coords_2d.y)* (1024)
            if x + (el['size']) *128 <= 30 or x - (el['size']) *128 >= 1000 or y + (el['size']) *128 <=30 or y - (el['size']) *128 >=1000:
              continue
            relsize=((el['size']) *128 + z)
            x1 = 0 if x - relsize < 0 else x - relsize
            y1 = 0 if y - relsize < 0 else y - relsize
            x2 = 1024 if x + relsize > 1024 else x + relsize
            y2 = 1024 if y + relsize > 1024 else y + relsize
            # if x1 < 0 or x2<0 or y1<0 or y2<0 or x1>=1024 or y1>=1024:
            #   continue
            flag = False
            repeats = []
            for el2 in tn:
              xA = max(x1, el2[0])
              yA = max(y1, el2[1])
              xB = min(x2, el2[2])
              yB = min(y2, el2[3])

              interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)
              boxAArea = (x2 - x1 + 1) * (y2 - y1 + 1)
              boxBArea = (el2[2] - el2[0] + 1) * (el2[3] - el2[1] + 1)
              inter1 = (interArea/boxAArea)*100
              inter2 = (interArea/boxBArea)*100
              if inter1 > 50 and el2[4]>el['z']:
                flag = True
                break
              if inter2 > 50 and el2[4] < el['z']:
                repeats.append(el2)
            if not flag:
              for element in repeats:
                tn.remove(element)
              tn.append([x1, y1, x2, y2, el['z'], el['size']])
            repeats.clear()



          num_objs = len(tn)
          tn2 = [elem[0:4] for elem in tn ]
          boxes = torch.tensor(tn2, dtype=torch.float)

          # there is only one class
          labels = torch.ones((num_objs,), dtype=torch.int64)

          image_id = idx
          area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])
          #print(area)
          # suppose all instances are not crowd
          iscrowd = torch.zeros((num_objs,), dtype=torch.int64)

          # Wrap sample and targets into torchvision tv_tensors:
          img = tv_tensors.Image(img)

          target = {}
          target["boxes"] = tv_tensors.BoundingBoxes(boxes, format="XYXY", canvas_size=F.get_size(img))
          #target["masks"] = tv_tensors.Mask(mask)
          target["labels"] = labels
          target["image_id"] = image_id
          target["area"] = area
          target["iscrowd"] = iscrowd

          if self.transforms is not None:
              img, target = self.transforms(img, target)

          return img, target

    def __len__(self):
        return len(self.imgs)

!wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py
!wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py
!wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py
!wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py
!wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py

from torchvision.transforms import v2 as T


def get_transform(train):
    transforms = []
    if train:
        transforms.append(T.RandomHorizontalFlip(0.5))
    transforms.append(T.ToDtype(torch.float, scale=True))
    transforms.append(T.ToPureTensor())
    return T.Compose(transforms)

import utils

model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights="DEFAULT")
dataset = PennFudanDataset('/content/drive/MyDrive/Dataset', None)
data_loader = torch.utils.data.DataLoader(
    dataset,
    batch_size=2,
    shuffle=True,
    num_workers=2,
    collate_fn=utils.collate_fn
)

# For Training
images, targets = next(iter(data_loader))
images = list(image.float() for image in images)
targets = [{k: v for k, v in t.items()} for t in targets]
output = model(images, targets)  # Returns losses and detections
print(output)

# For inference
model.eval()
x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]
predictions = model(x)  # Returns predictions
print(predictions[0])

os.environ['CUDA_LAUNCH_BLOCKING'] = '1'

os.environ["CUDA_VISIBLE_DEVICES"] = "1"

from engine import train_one_epoch, evaluate

# train on the GPU or on the CPU, if a GPU is not available
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model = torch.load("/content/drive/MyDrive/Dataset/model2.pth")
# our dataset has two classes only - background and person
num_classes = 2

dataset = PennFudanDataset('train', None)
dataset_test = PennFudanDataset('test', None)


# define training and validation data loaders
data_loader = torch.utils.data.DataLoader(
    dataset,
    batch_size=20,
    shuffle=True,
    num_workers=8,
    collate_fn=utils.collate_fn
)

data_loader_test = torch.utils.data.DataLoader(
    dataset_test,
    batch_size=1,
    shuffle=False,
    num_workers=8,
    collate_fn=utils.collate_fn
)

# get the model using our helper function
#model = get_model_instance_segmentation(num_classes)

# move model to the right device
model.to(device)

# construct an optimizer
params = [p for p in model.parameters() if p.requires_grad]
# optimizer = torch.optim.SGD(
#     params,
#     lr=0.005,
#     momentum=0.9,
#     weight_decay=0.0005
# )

optimizer = torch.optim.SGD(
    params,
    lr=0.0001,
    momentum=0.9,
    weight_decay=0.05
)

# and a learning rate scheduler
lr_scheduler = torch.optim.lr_scheduler.StepLR(
    optimizer,
    step_size=3,
    gamma=0.1
)

# let's train it just for 2 epochs
num_epochs = 10

for epoch in range(num_epochs):
    # train for one epoch, printing every 10 iterations
    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)
    # update the learning rate
    lr_scheduler.step()
    # evaluate on the test dataset
    evaluate(model, data_loader_test, device=device)

    torch.save(model.state_dict(), '/content/drive/MyDrive/Dataset/model3.pth')
    torch.save(model, '/content/drive/MyDrive/Dataset/model_new.pth')

from engine import train_one_epoch, evaluate

# train on the GPU or on the CPU, if a GPU is not available
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

# our dataset has two classes only - background and person
num_classes = 2
# use our dataset and defined transformations'/content/drive/MyDrive/Dataset', None
dataset = PennFudanDataset('train', None)
dataset_test = PennFudanDataset('test', None)



# define training and validation data loaders
data_loader = torch.utils.data.DataLoader(
    dataset,
    batch_size=10,
    shuffle=True,
    num_workers=2,
    collate_fn=utils.collate_fn
)

data_loader_test = torch.utils.data.DataLoader(
    dataset_test,
    batch_size=1,
    shuffle=False,
    num_workers=2,
    collate_fn=utils.collate_fn
)

# get the model using our helper function
#model = get_model_instance_segmentation(num_classes)

# move model to the right device
model.to(device)

# construct an optimizer
params = [p for p in model.parameters() if p.requires_grad]
# optimizer = torch.optim.SGD(
#     params,
#     lr=0.005,
#     momentum=0.9,
#     weight_decay=0.0005
# )

optimizer = torch.optim.SGD(
    params,
    lr=0.00001,
    momentum=0.9,
    weight_decay=0.05
)

# and a learning rate scheduler
lr_scheduler = torch.optim.lr_scheduler.StepLR(
    optimizer,
    step_size=3,
    gamma=0.1
)

# let's train it just for 2 epochs
num_epochs = 2

for epoch in range(num_epochs):
    # train for one epoch, printing every 10 iterations
    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)
    # update the learning rate
    lr_scheduler.step()
    # evaluate on the test dataset
    evaluate(model, data_loader_test, device=device)

print("That's it!")

num_epochs = 3

for epoch in range(num_epochs):
    # train for one epoch, printing every 10 iterations
    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)
    # update the learning rate
    lr_scheduler.step()
    # evaluate on the test dataset
    evaluate(model, data_loader_test, device=device)

# Commented out IPython magic to ensure Python compatibility.
# %tb

from engine import train_one_epoch, evaluate

# train on the GPU or on the CPU, if a GPU is not available
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model = torch.load("/content/drive/MyDrive/Dataset/model_new.pth", map_location=torch.device('cpu'))
model.eval()

!pip install bpy

!pip install --upgrade pip

!pip install bpy_extras

import torch
import numpy as np
import matplotlib.pyplot as plt
import json
import torchvision.transforms.functional as F


plt.rcParams["savefig.bbox"] = 'tight'


def show(imgs):
    if not isinstance(imgs, list):
        imgs = [imgs]
    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)
    for i, img in enumerate(imgs):
        img = img.detach()
        img = F.to_pil_image(img)
        axs[0, i].imshow(np.asarray(img))
        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])

from torchvision.utils import draw_bounding_boxes
from torchvision.utils import make_grid
from torchvision.io import read_image
from torchvision.io import read_image, ImageReadMode

# img = read_image("/content/drive/MyDrive/Dataset/particles8004.png", mode=ImageReadMode.RGB)
# show(img)

# annotations_file = open("/content/drive/MyDrive/Dataset/particles8004.json")

img = read_image("/content/drive/MyDrive/Dataset/renders/train/particles8000.png", mode=ImageReadMode.RGB)
show(img)

annotations_file = open("/content/drive/MyDrive/Dataset/particles/train/particles8000.json")
annotations = json.load(annotations_file)
img_filename = annotations['distribution']
points = annotations['particles']
size = []
tn = []
scale = np.array([11.0, 11.0, 2.0])
import bpy
import bpy_extras
from mathutils import Vector
# for el in points:
#   size.append(el['size'])
#   x = 512 + ((scale[0] * el['x'] - scale[0]/2) * 128)
#   y = 512 - ((scale[1] * el['y'] - scale[1]/2) * 128)
#   relsize=1
#   x1 = 0 if x - relsize < 0 else x - relsize
#   y1 = 0 if y - relsize < 0 else y - relsize
#   x2 = 1024 if x + relsize > 1024 else x + relsize
#   y2 = 1024 if y + relsize > 1024 else y + relsize
#   if x1 < 0 or x2<0 or y1<0 or y2<0 or x1>=1024 or y1>=1024:
#     continue
#   tn.append([x1, y1, x2, y2])
print(len(points))
cout = 0
i=6
j=0
siz=0
for el in points:
  size.append(el['size'])
# print(points[i]['x']*512)
# print(points[i]['y']*512)
# print(points[i]['z'])

# # u,v = xyz_to_uv(points[i]['x'], points[i]['y'], points[i]['z'])
# # print()
# # print(u * 1024)
# # print(v * 1024)
# print((scale[0] * (points[i]['x']+10) - scale[0]/2))
# print((scale[1] * (points[i]['y']+10) - scale[1]/2))
# print(((scale[0] * points[i]['x'] - scale[0]/2))*512-(10*scale[2] -scale[2]/2))
# print(((scale[1] * points[i]['y'] - scale[1]/2))*512)



# buf_x = points[i]['x'] + 5.5
# buf_y = points[i]['y'] + 5.5
# points[i]['x'] = 1
# points[i]['y'] = 1
# x = (((scale[0] * points[i]['x'] - scale[0]/2)) + 5.5) * (1024/11)
# y = 1024 - (((scale[1] * points[i]['y'] - scale[1]/2)) + 5.5) * (1024/11)



#x = points[i]['x']*1024
#y = 1024 - points[i]['y']*1024
  x = (scale[0] * (el['x']) - scale[0]/2)
  y = (scale[1] * (el['y']) - scale[1]/2)
  z = (scale[2] * (el['z']) - scale[2]/2)
  if i == 6:
    scene = bpy.data.scenes["Scene"]

# Set render resolution
    scene.render.resolution_x = 1024
    scene.render.resolution_y = 1024

# Set camera rotation in euler angles
    scene.camera.rotation_mode = 'XYZ'
    scene.camera.rotation_euler = (0.0, 0.0, 0.0)

# set the camera position
    scene.camera.location.x = 0
    scene.camera.location.y = 0
#scene.camera.location.z = 10

    scene.camera.location.z = 10

# first choose the cycles rendering engine
    bpy.context.scene.render.engine = 'CYCLES'
    bpy.ops.mesh.primitive_uv_sphere_add(
        #segments=64,
        #ring_count=32,
        radius=points[i]['size'],
        location=(x,y,z))
    i += 1
  #print(scene.camera.location)
  #   render_scale = scene.render.resolution_percentage / 100
  # render_size = (
  #   int(scene.render.resolution_x * render_scale),
  #   int(scene.render.resolution_y * render_scale),
  # )
#coord_3d = Vector((points[i]['x'], points[i]['y'], points[i]['z']))
  coord_3d = Vector((x, y, z))
  coords_2d = bpy_extras.object_utils.world_to_camera_view(scene, scene.camera, coord_3d)

  # print()
  # print(coords_2d.x)
  # print(coords_2d.y)
  # print(coords_2d.x*render_size[0])
  # print(coords_2d.y*render_size[1])
  # print((coords_2d.x) * (1024))
  # print((coords_2d.y)* (1024))
# x = round(coords_2d.x * 512)
# y = round((1-coords_2d.y)*512)
  x = (coords_2d.x) * (1024)
  y = (1 - coords_2d.y)* (1024)
  if x + (el['size']) *128 <= 30 or x - (el['size']) *128 >= 1000 or y + (el['size']) *128 <=30 or y - (el['size']) *128 >=1000:
    continue
  relsize=((el['size']) *128 + z)
  x1 = 0 if x - relsize < 0 else x - relsize
  y1 = 0 if y - relsize < 0 else y - relsize
  x2 = 1024 if x + relsize > 1024 else x + relsize
  y2 = 1024 if y + relsize > 1024 else y + relsize
  # if x1 < 0 or x2<0 or y1<0 or y2<0 or x1>=1024 or y1>=1024:
  #   continue
  flag = False
  repeats = []
  for el2 in tn:
    #print()
    xA = max(x1, el2[0])
    yA = max(y1, el2[1])
    xB = min(x2, el2[2])
    yB = min(y2, el2[3])

    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)
    #print(interArea)
    boxAArea = (x2 - x1 + 1) * (y2 - y1 + 1)
    boxBArea = (el2[2] - el2[0] + 1) * (el2[3] - el2[1] + 1)
    inter1 = (interArea/boxAArea)*100
    inter2 = (interArea/boxBArea)*100
    # print()
    # print(inter1, " - ", el2[4]-el['z'], " : ", el2[5]-el['size'])
    # print(inter2, " - ", el2[4]-el['z'], " : ", el2[5]-el['size'])
    #print(el2[4])
    #print(el['z'])
    #if inter1 > 50 and el2[4] + el2[5]>el['z']+el['size']:
    if inter1 > 50 and el2[4]>el['z']:
      flag = True
      break
    #if inter2 > 50 and el2[4] + el2[5] < el['z']+el['size']:
    if inter2 > 50 and el2[4] < el['z']:
      repeats.append(el2)
      #tn.remove(el2)

    # if inter1 > 50 and el2[4] + el2[5] - el['z'] - el['size'] > 0.01:
    #   flag = True
    #   j+=1
    #   break
  if not flag:
    for element in repeats:
      tn.remove(element)
    tn.append([x1, y1, x2, y2, el['z'], el['size']])
    #siz+=1
    #print("siz", siz)
  repeats.clear()



#print(siz)
#print(j)
num_objs = len(size)
#print(tn)
tn2 = [elem[0:4] for elem in tn ]
print(tn2)
boxes = torch.tensor(tn2, dtype=torch.float)

#print(boxes)
result = draw_bounding_boxes(img, boxes, colors="red", width=10)
show(result)

import matplotlib.pyplot as plt

from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks
from torchvision.io import read_image, ImageReadMode
from torchvision.ops.boxes import masks_to_boxes
from torchvision import tv_tensors

image = read_image("/content/drive/MyDrive/Dataset/renders/test/particles8911.png", mode=ImageReadMode.RGB).float()
img= tv_tensors.Image(image)
#eval_transform = get_transform(train=False)

model.eval()
with torch.no_grad():
    #x = eval_transform(image)
    # convert RGBA -> RGB and move to device
    img = img.to(device)
    predictions = model([img])
    pred = predictions[0]


image = (255.0 * (image - image.min()) / (image.max() - image.min())).to(torch.uint8)
image = image[:3, ...]
pred_labels = [f"pedestrian: {score:.3f}" for label, score in zip(pred["labels"], pred["scores"])]
pred_boxes = pred["boxes"].long()
output_image = draw_bounding_boxes(image, pred_boxes, pred_labels, colors="red")

#masks = (pred["masks"] > 0.7).squeeze(1)
#output_image = draw_segmentation_masks(output_image, masks, alpha=0.5, colors="blue")


plt.figure(figsize=(12, 12))
plt.imshow(output_image.permute(1, 2, 0))

!pip install bpy

import os
images_train_path = list(sorted(os.listdir("/content/drive/MyDrive/Dataset/renders/train")))
images_train_path = images_train_path[:2]

for img_path1 in images_train_path:
    img_path = os.path.join("/content/drive/MyDrive/Dataset/renders/train", img_path1)
    print(img_path)
    ch = "."
    ch_index = img_path1.find(ch)
    annot = img_path1[:ch_index]
    print(annot)
    annot_path = f"/content/drive/MyDrive/Dataset/particles/train/{annot}.json"
    print(annot_path)

import torch
import numpy as np
import matplotlib.pyplot as plt
import json
import torchvision.transforms.functional as F


plt.rcParams["savefig.bbox"] = 'tight'


# def show(imgs):
#     if not isinstance(imgs, list):
#         imgs = [imgs]
#     fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)
#     for i, img in enumerate(imgs):
#         img = img.detach()
#         img = F.to_pil_image(img)
#         axs[0, i].imshow(np.asarray(img))
#         axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])

from torchvision.utils import draw_bounding_boxes
from torchvision.utils import make_grid
from torchvision.io import read_image
from torchvision.io import read_image, ImageReadMode


images_train_path = list(sorted(os.listdir("/content/drive/MyDrive/Dataset/renders/test")))
images_train_path = images_train_path[:1500]
all_diameters = []
coy = 1
for img_path1 in images_train_path:
    img_path = os.path.join("/content/drive/MyDrive/Dataset/renders/test", img_path1)
    ch = "."
    ch_index = img_path1.find(ch)
    annot = img_path1[:ch_index]
    annot_path = f"/content/drive/MyDrive/Dataset/particles/test/{annot}.json"
    print(annot_path)
    img = read_image(img_path, mode=ImageReadMode.RGB)
    #show(img)

    #annotations_file = open("/content/drive/MyDrive/Dataset/particles/test/particles8902.json")
    annotations_file = open(annot_path)
    annotations = json.load(annotations_file)
    img_filename = annotations['distribution']
    points = annotations['particles']
    size = []
    tn = []
    coy+=1

    diameters = []

    scale = np.array([11.0, 11.0, 2.0])
    import bpy
    import bpy_extras
    from mathutils import Vector

    #print(len(points))
    cout = 0
    i=6
    j=0
    siz=0
    for el in points:
      size.append(el['size'])

      x = (scale[0] * (el['x']) - scale[0]/2)
      y = (scale[1] * (el['y']) - scale[1]/2)
      z = (scale[2] * (el['z']) - scale[2]/2)
      if i == 6:
        scene = bpy.data.scenes["Scene"]

    # Set render resolution
        scene.render.resolution_x = 1024
        scene.render.resolution_y = 1024

    # Set camera rotation in euler angles
        scene.camera.rotation_mode = 'XYZ'
        scene.camera.rotation_euler = (0.0, 0.0, 0.0)

    # set the camera position
        scene.camera.location.x = 0
        scene.camera.location.y = 0
        scene.camera.location.z = 10

        bpy.context.scene.render.engine = 'CYCLES'
        bpy.ops.mesh.primitive_uv_sphere_add(
            radius=points[i]['size'],
            location=(x,y,z))
        i += 1

      coord_3d = Vector((x, y, z))
      coords_2d = bpy_extras.object_utils.world_to_camera_view(scene, scene.camera, coord_3d)

      x = (coords_2d.x) * (1024)
      y = (1 - coords_2d.y)* (1024)
      if x + (el['size']) *128 <= 30 or x - (el['size']) *128 >= 1000 or y + (el['size']) *128 <=30 or y - (el['size']) *128 >=1000:
        continue
      relsize=((el['size']) *128 + z)
      x1 = 0 if x - relsize < 0 else x - relsize
      y1 = 0 if y - relsize < 0 else y - relsize
      x2 = 1024 if x + relsize > 1024 else x + relsize
      y2 = 1024 if y + relsize > 1024 else y + relsize
      flag = False
      repeats = []
      repeats2 = []
      for el2 in tn:
        #print()
        xA = max(x1, el2[0])
        yA = max(y1, el2[1])
        xB = min(x2, el2[2])
        yB = min(y2, el2[3])

        interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)
        boxAArea = (x2 - x1 + 1) * (y2 - y1 + 1)
        boxBArea = (el2[2] - el2[0] + 1) * (el2[3] - el2[1] + 1)
        inter1 = (interArea/boxAArea)*100
        inter2 = (interArea/boxBArea)*100

        if inter1 > 50 and el2[4]>el['z']:
          flag = True
          break
        if inter2 > 50 and el2[4] < el['z']:
          repeats.append(el2)

      if not flag:
        for element in repeats:
          tn.remove(element)
        tn.append([x1, y1, x2, y2, el['z'], el['size']])
        #diameters.append(relsize*2)
      repeats.clear()

    num_objs = len(size)
    tn2 = [elem[0:4] for elem in tn ]
    diameters = [(elem[5]*128)*2 for elem in tn]
    all_diameters.append(diameters)
    #print(tn2)
    # print(len(tn))
    # print(len(tn2))
    # print(len(diameters))
    #boxes = torch.tensor(tn2, dtype=torch.float)

#result = draw_bounding_boxes(img, boxes, colors="red", width=10)
#show(result)

print(len(all_diameters))

import matplotlib.pyplot as plt
import os
from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks
from torchvision.io import read_image, ImageReadMode
from torchvision.ops.boxes import masks_to_boxes
from torchvision import tv_tensors


images_train_path = list(sorted(os.listdir("/content/drive/MyDrive/Dataset/renders/test")))
images_train_path = images_train_path[:1500]


all_pred_boxes = []
for img in images_train_path:
  img_path = os.path.join("/content/drive/MyDrive/Dataset/renders/test", img)
  print(img_path)
  image = read_image(img_path, mode=ImageReadMode.RGB).float()
  img= tv_tensors.Image(image)
  #eval_transform = get_transform(train=False)

  model.eval()
  with torch.no_grad():
      #x = eval_transform(image)
      # convert RGBA -> RGB and move to device
      img = img.to(device)
      predictions = model([img])
      pred = predictions[0]


  #image = (255.0 * (image - image.min()) / (image.max() - image.min())).to(torch.uint8)
  #image = image[:3, ...]
  #pred_labels = [f"pedestrian: {score:.3f}" for label, score in zip(pred["labels"], pred["scores"])]
  pred_boxes = pred["boxes"].long()
  pred_boxes_cpu=pred_boxes.cpu()
  pred_max = []
  for el in pred_boxes_cpu:
    pred_max.append(max(el[2]-el[0], el[3]-el[1]))
  all_pred_boxes.append(pred_max)
  #output_image = draw_bounding_boxes(image, pred_boxes, pred_labels, colors="red")

#masks = (pred["masks"] > 0.7).squeeze(1)
#output_image = draw_segmentation_masks(output_image, masks, alpha=0.5, colors="blue")


# plt.figure(figsize=(12, 12))
# plt.imshow(output_image.permute(1, 2, 0))

import matplotlib.pyplot as plt
import numpy as np
# np.random.seed(42)
# x = np.random.normal(size=1000)

random_val = []
random_val2 = []
for diameters in all_diameters:
  #diam, bn, _ = plt.hist(diameters, bins=40)  # density=False would make counts
  (hst, bin_edges) = np.histogram(diameters, bins=10)
  #plt.ylabel('Probability')
  #plt.xlabel('Data');
  random_values = np.random.choice(bin_edges[:-1], size=500, p=hst/sum(hst))
  random_val.append(random_values)
  #print(random_values)

for pred_max in all_pred_boxes:
  (hst2, bin_edges2) = np.histogram(pred_max, bins=10)
  random_values2 = np.random.choice(bin_edges2[:-1], size=500, p=hst2/sum(hst2))
  random_val2.append(random_values2)

kl_div1 = 0
kl_div2 = 0
for i in range(len(random_val)):
  # Ensuring all values are positive
  hist2 = np.abs(random_val2[i]).astype('float64')
  #print(hist2)
  diameters2 = np.abs(random_val[i]).astype('float64')
  #print(diameters2)
  # Normalize to form a valid probability distribution
  hist2 /= np.sum(hist2)
  diameters2 /= np.sum(diameters2)
  #print(hist2)
  #print(diameters2)
  # Now, Calculate the KL Divergence
  kl_divergence = np.sum(diameters2 * np.log(diameters2 / hist2))
  kl_div1 += kl_divergence
  #print("KL Divergence:", kl_divergence)


  # Ensuring all values are positive
  hist2 = np.abs(random_val2[i]).astype('float64')
  #print(hist2)
  diameters2 = np.abs(random_val[i]).astype('float64')
  #print(diameters2)
  # Normalize to form a valid probability distribution
  hist2 /= np.sum(hist2)
  diameters2 /= np.sum(diameters2)
  #print(hist2)
  #print(diameters2)
  # Now, Calculate the KL Divergence
  kl_divergence = np.sum(hist2 * np.log(hist2 / diameters2))
  kl_div2 += kl_divergence
  #print("KL Divergence:", kl_divergence)

print(kl_div1/len(random_val))
print(kl_div2/len(random_val))

import matplotlib.pyplot as plt
import numpy as np
# np.random.seed(42)
# x = np.random.normal(size=1000)

diam, bn, _ = plt.hist(diameters, bins=40)  # density=False would make counts
(hst, bin_edges) = np.histogram(diameters, bins=40)
plt.ylabel('Probability')
plt.xlabel('Data');

random_values = np.random.choice(bin_edges[:-1], size=500, p=hst/sum(hst))
print(random_values)

pred_boxes_cpu=pred_boxes.cpu()

pred_max = []
for el in pred_boxes_cpu:
  pred_max.append(max(el[2]-el[0], el[3]-el[1]))

print(pred_max)

print(len(diameters))

(hst2, bin_edges2) = np.histogram(pred_max, bins=40)
plt.hist(pred_max, bins=40)

random_values2 = np.random.choice(bin_edges2[:-1], size=500, p=hst2/sum(hst2))
print(random_values2)

from scipy. special import rel_entr

sum(rel_entr(random_values, random_values2))

from scipy.special import kl_div

kl_PQ = kl_div(random_values, random_values2).sum()
print(kl_PQ)

# Ensuring all values are positive
hist2 = np.abs(random_values2).astype('float64')
#print(hist2)
diameters2 = np.abs(random_values).astype('float64')
#print(diameters2)
# Normalize to form a valid probability distribution
hist2 /= np.sum(hist2)
diameters2 /= np.sum(diameters2)
#print(hist2)
#print(diameters2)
# Now, Calculate the KL Divergence
kl_divergence = np.sum(diameters2 * np.log(diameters2 / hist2))
print("KL Divergence:", kl_divergence)

# Ensuring all values are positive
hist2 = np.abs(random_values2).astype('float64')
#print(hist2)
diameters2 = np.abs(random_values).astype('float64')
#print(diameters2)
# Normalize to form a valid probability distribution
hist2 /= np.sum(hist2)
diameters2 /= np.sum(diameters2)
#print(hist2)
#print(diameters2)
# Now, Calculate the KL Divergence
kl_divergence = np.sum(hist2 * np.log(hist2 / diameters2))
print("KL Divergence:", kl_divergence)

from scipy.stats import ks_2samp
p_value=ks_2samp(hst, hst2).pvalue
print(p_value)

from scipy.stats import mannwhitneyu
p_value=mannwhitneyu(hst, hst2).pvalue
print(p_value)

from scipy.stats import chisquare
p_value=chisquare(hst, hst2).pvalue
print(p_value)

import seaborn as sns
import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(16, 10))
# Bar plot
# sns.barplot(x=np.arange(len(random_values)), y=random_values, label='p (Bar)', color='blue', alpha=0.5, ax=ax)
# sns.barplot(x=np.arange(len(random_values2)), y=random_values2, label='q (Bar)', color='red', alpha=0.5, ax=ax)
# sns.barplot(x=hst, y=len(hst), label='p (Bar)', color='blue', alpha=0.5, ax=ax)
# sns.barplot(x=hst2, y=len(hst), label='q (Bar)', color='red', alpha=0.5, ax=ax)
# Line plot
sns.lineplot(x=np.arange(len(hst)), y=hst, label='diameters (Line)', color='darkgreen', ax=ax,linewidth = 10)
sns.lineplot(x=np.arange(len(hst2)), y=hst2, label='bounding boxes (Line)', color='black', ax=ax,linewidth = 10)
plt.title('Probability Density Functions')
plt.xlabel('Index')
plt.ylabel('Value')
plt.legend()
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(16, 10))
# Bar plot
# sns.barplot(x=np.arange(len(random_values)), y=random_values, label='p (Bar)', color='blue', alpha=0.5, ax=ax)
# sns.barplot(x=np.arange(len(random_values2)), y=random_values2, label='q (Bar)', color='red', alpha=0.5, ax=ax)
# sns.barplot(x=hst, y=len(hst), label='p (Bar)', color='blue', alpha=0.5, ax=ax)
# sns.barplot(x=hst2, y=len(hst), label='q (Bar)', color='red', alpha=0.5, ax=ax)
# Line plot
sns.lineplot(x=np.arange(len(hst)), y=hst-hst2, label='разность двух распределений (Line)', color='darkgreen', ax=ax,linewidth = 10)
#sns.lineplot(x=np.arange(len(hst2)), y=hst2, label='bounding boxes (Line)', color='black', ax=ax,linewidth = 10)
plt.title('Probability Density Functions')
plt.xlabel('Index')
plt.ylabel('Value')
plt.legend()
plt.show()

import matplotlib.pyplot as plt

from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks
from torchvision.io import read_image, ImageReadMode
from torchvision.ops.boxes import masks_to_boxes
from torchvision import tv_tensors

image = read_image("/content/drive/MyDrive/Dataset/renders/test/particles9000.png", mode=ImageReadMode.RGB).float()
img= tv_tensors.Image(image)
#eval_transform = get_transform(train=False)

model.eval()
with torch.no_grad():
    #x = eval_transform(image)
    # convert RGBA -> RGB and move to device
    img = img.to(device)
    predictions = model([img])
    pred = predictions[0]


image = (255.0 * (image - image.min()) / (image.max() - image.min())).to(torch.uint8)
image = image[:3, ...]
pred_labels = [f"pedestrian: {score:.3f}" for label, score in zip(pred["labels"], pred["scores"])]
pred_boxes = pred["boxes"].long()
output_image = draw_bounding_boxes(image, pred_boxes, pred_labels, colors="red")

#masks = (pred["masks"] > 0.7).squeeze(1)
#output_image = draw_segmentation_masks(output_image, masks, alpha=0.5, colors="blue")


plt.figure(figsize=(12, 12))
plt.imshow(output_image.permute(1, 2, 0))

import torch
import numpy as np
import matplotlib.pyplot as plt
import json
import torchvision.transforms.functional as F


plt.rcParams["savefig.bbox"] = 'tight'


def show(imgs):
    if not isinstance(imgs, list):
        imgs = [imgs]
    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)
    for i, img in enumerate(imgs):
        img = img.detach()
        img = F.to_pil_image(img)
        axs[0, i].imshow(np.asarray(img))
        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])

from torchvision.utils import draw_bounding_boxes
from torchvision.utils import make_grid
from torchvision.io import read_image
from torchvision.io import read_image, ImageReadMode


img = read_image("/content/drive/MyDrive/Dataset/renders/test/particles9000.png", mode=ImageReadMode.RGB)
show(img)

annotations_file = open("/content/drive/MyDrive/Dataset/particles/test/particles9000.json")
annotations = json.load(annotations_file)
img_filename = annotations['distribution']
points = annotations['particles']
size = []
tn = []

diameters = []

scale = np.array([11.0, 11.0, 2.0])
import bpy
import bpy_extras
from mathutils import Vector

print(len(points))
cout = 0
i=6
j=0
siz=0
for el in points:
  size.append(el['size'])

  x = (scale[0] * (el['x']) - scale[0]/2)
  y = (scale[1] * (el['y']) - scale[1]/2)
  z = (scale[2] * (el['z']) - scale[2]/2)
  if i == 6:
    scene = bpy.data.scenes["Scene"]

# Set render resolution
    scene.render.resolution_x = 1024
    scene.render.resolution_y = 1024

# Set camera rotation in euler angles
    scene.camera.rotation_mode = 'XYZ'
    scene.camera.rotation_euler = (0.0, 0.0, 0.0)

# set the camera position
    scene.camera.location.x = 0
    scene.camera.location.y = 0
    scene.camera.location.z = 10

    bpy.context.scene.render.engine = 'CYCLES'
    bpy.ops.mesh.primitive_uv_sphere_add(
        radius=points[i]['size'],
        location=(x,y,z))
    i += 1

  coord_3d = Vector((x, y, z))
  coords_2d = bpy_extras.object_utils.world_to_camera_view(scene, scene.camera, coord_3d)

  x = (coords_2d.x) * (1024)
  y = (1 - coords_2d.y)* (1024)
  if x + (el['size']) *128 <= 30 or x - (el['size']) *128 >= 1000 or y + (el['size']) *128 <=30 or y - (el['size']) *128 >=1000:
    continue
  relsize=((el['size']) *128 + z)
  x1 = 0 if x - relsize < 0 else x - relsize
  y1 = 0 if y - relsize < 0 else y - relsize
  x2 = 1024 if x + relsize > 1024 else x + relsize
  y2 = 1024 if y + relsize > 1024 else y + relsize
  flag = False
  repeats = []
  repeats2 = []
  for el2 in tn:
    #print()
    xA = max(x1, el2[0])
    yA = max(y1, el2[1])
    xB = min(x2, el2[2])
    yB = min(y2, el2[3])

    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)
    boxAArea = (x2 - x1 + 1) * (y2 - y1 + 1)
    boxBArea = (el2[2] - el2[0] + 1) * (el2[3] - el2[1] + 1)
    inter1 = (interArea/boxAArea)*100
    inter2 = (interArea/boxBArea)*100

    if inter1 > 50 and el2[4]>el['z']:
      flag = True
      break
    if inter2 > 50 and el2[4] < el['z']:
      repeats.append(el2)

  if not flag:
    for element in repeats:
      tn.remove(element)
    tn.append([x1, y1, x2, y2, el['z'], el['size']])
    diameters.append(relsize*2)
  repeats.clear()

num_objs = len(size)
tn2 = [elem[0:4] for elem in tn ]
diameters = [(elem[4]*128 + elem[5])*2 for elem in tn]
#print(tn2)

print(len(tn))
print(len(tn2))
print(len(diameters))
boxes = torch.tensor(tn2, dtype=torch.float)

result = draw_bounding_boxes(img, boxes, colors="red", width=10)
show(result)

import matplotlib.pyplot as plt
import numpy as np
# np.random.seed(42)
# x = np.random.normal(size=1000)

diam, bn, _ = plt.hist(diameters, bins=40)  # density=False would make counts
(hst, bin_edges) = np.histogram(diameters, bins=40)
# plt.ylabel('Probability')
# plt.xlabel('Data');

pred_boxes_cpu=pred_boxes.cpu()

hst3, bn3, _ = plt.hist(pred_boxes_cpu, bins=40)  # density=False would make counts
(hst2, bin_edges2) = np.histogram(pred_boxes_cpu, bins=40)
#plt.hist(hst2, bins=40)

from scipy. special import rel_entr

print(sum(rel_entr(hst, hst2)))
print(sum(rel_entr(hst2, hst)))

from scipy.special import kl_div

kl_PQ = kl_div(hst, hst2).sum()
print(kl_PQ)

kl_PQ = kl_div(hst2, hst).sum()
print(kl_PQ)

# Ensuring all values are positive
hist2 = np.abs(hst2).astype('float64')
#print(hist2)
diameters2 = np.abs(hst).astype('float64')
#print(diameters2)
# Normalize to form a valid probability distribution
hist2 /= np.sum(hist2)
diameters2 /= np.sum(diameters2)
#print(hist2)
#print(diameters2)
# Now, Calculate the KL Divergence
kl_divergence = np.sum(diameters2 * np.log(diameters2 / hist2))
print("KL Divergence:", kl_divergence)

# Ensuring all values are positive
hist2 = np.abs(hst2).astype('float64')
#print(hist2)
diameters2 = np.abs(hst).astype('float64')
#print(diameters2)
# Normalize to form a valid probability distribution
hist2 /= np.sum(hist2)
diameters2 /= np.sum(diameters2)
#print(hist2)
#print(diameters2)
# Now, Calculate the KL Divergence
kl_divergence = np.sum(hist2 * np.log(hist2 / diameters2))
print("KL Divergence:", kl_divergence)