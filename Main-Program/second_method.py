# -*- coding: utf-8 -*-
"""second method.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fZev72yWeHJETOH3v0VRZHRJxdRz2Pxr
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install efficientnet-pytorch

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from efficientnet_pytorch import EfficientNet
from torch.utils.data import DataLoader

import numpy as np
import os
from torchvision import tv_tensors
from torchvision.io import read_image, ImageReadMode
import json

#Define the dataset

class HistogramDataset(torch.utils.data.Dataset):
  def __init__(self, root):#images, histograms):
    self.root = root
    # self.images = images
    # self.histograms = histograms
    self.imgs = list(sorted(os.listdir(os.path.join("/content/drive/MyDrive/Dataset/renders/", root))))
    self.masks = list(sorted(os.listdir(os.path.join("/content/drive/MyDrive/Dataset/particles/", root))))

  def __len__(self):
    return len(self.imgs)

  def __getitem__(self, idx):
    # image = self.images[idx]
    # histogram = self.histograms[idx]
    # return image, histogram
    img_path = os.path.join("/content/drive/MyDrive/Dataset/renders/", self.root, self.imgs[idx])
    mask_path = os.path.join("/content/drive/MyDrive/Dataset/renders/", self.root, self.imgs[idx])
    # img_path = os.path.join(self.root, "train", self.imgs[idx])
    # mask_path = os.path.join(self.root, "train", self.imgs[idx])
    img = read_image(img_path, mode=ImageReadMode.RGB).float()
    #mask = read_image(mask_path)
    annotations_file = open(os.path.join("/content/drive/MyDrive/Dataset/particles/", self.root, self.masks[idx]))
    #annotations_file = open(os.path.join(self.root, "json", self.masks[idx]))
    annotations = json.load(annotations_file)
    img_filename = annotations['distribution']
    points = annotations['particles']

    size = []
    tn = []
    diameters = []
    scale = np.array([11.0, 11.0, 2.0])
    import bpy
    import bpy_extras
    from mathutils import Vector

    cout = 0
    i=6
    j=0
    siz=0
    for el in points:
      size.append(el['size'])

      x = (scale[0] * (el['x']) - scale[0]/2)
      y = (scale[1] * (el['y']) - scale[1]/2)
      z = (scale[2] * (el['z']) - scale[2]/2)
      if i == 6:
        scene = bpy.data.scenes["Scene"]

  # Set render resolution
        scene.render.resolution_x = 1024
        scene.render.resolution_y = 1024

  # Set camera rotation in euler angles
        scene.camera.rotation_mode = 'XYZ'
        scene.camera.rotation_euler = (0.0, 0.0, 0.0)

  # set the camera position
        scene.camera.location.x = 0
        scene.camera.location.y = 0
        scene.camera.location.z = 10

        bpy.context.scene.render.engine = 'CYCLES'
        bpy.ops.mesh.primitive_uv_sphere_add(
            radius=points[i]['size'],
            location=(x,y,z))
        i += 1

      coord_3d = Vector((x, y, z))
      coords_2d = bpy_extras.object_utils.world_to_camera_view(scene, scene.camera, coord_3d)

      x = (coords_2d.x) * (1024)
      y = (1 - coords_2d.y)* (1024)
      if x + (el['size']) *128 <= 30 or x - (el['size']) *128 >= 1000 or y + (el['size']) *128 <=30 or y - (el['size']) *128 >=1000:
        continue
      relsize=((el['size']) *128 + z)
      x1 = 0 if x - relsize < 0 else x - relsize
      y1 = 0 if y - relsize < 0 else y - relsize
      x2 = 1024 if x + relsize > 1024 else x + relsize
      y2 = 1024 if y + relsize > 1024 else y + relsize
      flag = False
      repeats = []
      repeats2 = []
      for el2 in tn:
        #print()
        xA = max(x1, el2[0])
        yA = max(y1, el2[1])
        xB = min(x2, el2[2])
        yB = min(y2, el2[3])

        interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)
        boxAArea = (x2 - x1 + 1) * (y2 - y1 + 1)
        boxBArea = (el2[2] - el2[0] + 1) * (el2[3] - el2[1] + 1)
        inter1 = (interArea/boxAArea)*100
        inter2 = (interArea/boxBArea)*100

        if inter1 > 50 and el2[4]>el['z']:
          flag = True
          break
        if inter2 > 50 and el2[4] < el['z']:
          repeats.append(el2)

      if not flag:
        for element in repeats:
          tn.remove(element)
        tn.append([x1, y1, x2, y2, el['z'], el['size']])
        #diameters.append(relsize*2)
      repeats.clear()

    num_objs = len(size)
    tn2 = [elem[0:4] for elem in tn ]
    diameters = [(elem[5]*128 + elem[4])*2 for elem in tn]
    #print(tn2)
    (hst, bin_edges) = np.histogram(diameters, bins=10)
    #image_train.append(img)
    hst_norm = hst/np.sum(hst)
    #histograms_train.append(hst_norm)
    img = tv_tensors.Image(img)
    return img, hst_norm

class HistogramValDataset(torch.utils.data.Dataset):
  def __init__(self, root):#images, histograms):
    self.root = root
    # self.images = images
    # self.histograms = histograms
    self.imgs = list(sorted(os.listdir(os.path.join("/content/drive/MyDrive/Dataset/renders/", root))))
    self.masks = list(sorted(os.listdir(os.path.join("/content/drive/MyDrive/Dataset/particles/", root))))

  def __len__(self):
    return len(self.imgs)

  def __getitem__(self, idx):
    # image = self.images[idx]
    # histogram = self.histograms[idx]
    # return image, histogram
    img_path = os.path.join("/content/drive/MyDrive/Dataset/renders/", self.root, self.imgs[idx])
    mask_path = os.path.join("/content/drive/MyDrive/Dataset/renders/", self.root, self.imgs[idx])
    # img_path = os.path.join(self.root, "train", self.imgs[idx])
    # mask_path = os.path.join(self.root, "train", self.imgs[idx])
    img = read_image(img_path, mode=ImageReadMode.RGB).float()
    #mask = read_image(mask_path)
    annotations_file = open(os.path.join("/content/drive/MyDrive/Dataset/particles/", self.root, self.masks[idx]))
    #annotations_file = open(os.path.join(self.root, "json", self.masks[idx]))
    annotations = json.load(annotations_file)
    img_filename = annotations['distribution']
    points = annotations['particles']

    size = []
    tn = []
    diameters = []
    scale = np.array([11.0, 11.0, 2.0])
    import bpy
    import bpy_extras
    from mathutils import Vector

    cout = 0
    i=6
    j=0
    siz=0
    for el in points:
      size.append(el['size'])

      x = (scale[0] * (el['x']) - scale[0]/2)
      y = (scale[1] * (el['y']) - scale[1]/2)
      z = (scale[2] * (el['z']) - scale[2]/2)
      if i == 6:
        scene = bpy.data.scenes["Scene"]

  # Set render resolution
        scene.render.resolution_x = 1024
        scene.render.resolution_y = 1024

  # Set camera rotation in euler angles
        scene.camera.rotation_mode = 'XYZ'
        scene.camera.rotation_euler = (0.0, 0.0, 0.0)

  # set the camera position
        scene.camera.location.x = 0
        scene.camera.location.y = 0
        scene.camera.location.z = 10

        bpy.context.scene.render.engine = 'CYCLES'
        bpy.ops.mesh.primitive_uv_sphere_add(
            radius=points[i]['size'],
            location=(x,y,z))
        i += 1

      coord_3d = Vector((x, y, z))
      coords_2d = bpy_extras.object_utils.world_to_camera_view(scene, scene.camera, coord_3d)

      x = (coords_2d.x) * (1024)
      y = (1 - coords_2d.y)* (1024)
      if x + (el['size']) *128 <= 30 or x - (el['size']) *128 >= 1000 or y + (el['size']) *128 <=30 or y - (el['size']) *128 >=1000:
        continue
      relsize=((el['size']) *128 + z)
      x1 = 0 if x - relsize < 0 else x - relsize
      y1 = 0 if y - relsize < 0 else y - relsize
      x2 = 1024 if x + relsize > 1024 else x + relsize
      y2 = 1024 if y + relsize > 1024 else y + relsize
      flag = False
      repeats = []
      repeats2 = []
      for el2 in tn:
        #print()
        xA = max(x1, el2[0])
        yA = max(y1, el2[1])
        xB = min(x2, el2[2])
        yB = min(y2, el2[3])

        interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)
        boxAArea = (x2 - x1 + 1) * (y2 - y1 + 1)
        boxBArea = (el2[2] - el2[0] + 1) * (el2[3] - el2[1] + 1)
        inter1 = (interArea/boxAArea)*100
        inter2 = (interArea/boxBArea)*100

        if inter1 > 50 and el2[4]>el['z']:
          flag = True
          break
        if inter2 > 50 and el2[4] < el['z']:
          repeats.append(el2)

      if not flag:
        for element in repeats:
          tn.remove(element)
        tn.append([x1, y1, x2, y2, el['z'], el['size']])
        #diameters.append(relsize*2)
      repeats.clear()

    num_objs = len(size)
    tn2 = [elem[0:4] for elem in tn ]
    diameters = [(elem[5]*128 + elem[4])*2 for elem in tn]
    #print(tn2)
    (hst, bin_edges) = np.histogram(diameters, bins=10)
    #image_train.append(img)
    hst_norm = hst/np.sum(hst)
    #histograms_train.append(hst_norm)
    img = tv_tensors.Image(img)
    return img, hst_norm

#Define the model

class HistogramClassifier(nn.Module):
  def __init__(self, num_classes):
    super(HistogramClassifier, self).__init__()
    self.backbone = EfficientNet.from_pretrained('efficientnet-b0')
    self.fc = nn.Linear(1280, num_classes)

  def forward(self, x):
    x = self.backbone.extract_features(x)
    x = nn.functional.adaptive_avg_pool2d(x, 1).reshape(x.shape[0], -1)
    x = self.fc(x)
    return x

#Define the training loop
def train(model, dataloader, criterion, optimizer):
  model.train()
  #correct = 0
  #total = 0
  running_loss = 0.0
  for i, (images, histograms) in enumerate(dataloader):
    images = images.to(torch.device('cuda:0'))
    histograms = histograms.to(torch.device('cuda:0'))
    optimizer.zero_grad()
    outputs = model(images)
    loss = criterion(outputs, histograms)
    loss.backward()
    optimizer.step()
    running_loss += loss.item()

    _, predicted = torch.max(outputs.data, 1)
    #total += histograms.size(0)
    #correct += (predicted == histograms).sum().item()
  #acc = correct/total
  #print(f'Accuracy = {acc:.4f}')
  return running_loss/len(dataloader)

#Define the validation loop
def validate(model, dataloader, criterion):
  model.eval()
  #correct = 0
  #total = 0
  running_loss = 0.0
  with torch.no_grad():
    for i, (images, histograms) in enumerate(dataloader):
      images = images.to(torch.device('cuda:0'))
      histograms = histograms.to(torch.device('cuda:0'))
      outputs = model(images)
      loss = criterion(outputs, histograms)
      running_loss += loss.item()

      _, predicted = torch.max(outputs.data, 1)
      #total += histograms.size(0)
      #correct += (predicted == histograms).sum().item()
  #acc = correct/total
  #print(f'Accuracy = {acc:.4f}')
  return running_loss/len(dataloader)

#Define the test loop
def test(model, dataloader):
  model.eval()
  correct = 0
  total = 0
  with torch.no_grad():
    for i, (images, histograms) in enumerate(dataloader):
      images = images.to(torch.device('cuda:0'))
      histograms = histograms.to(torch.device('cuda:0'))
      outputs = model(images)
      _, predicted = torch.max(outputs.data, 1)
      total += histograms.size(0)
      correct += (predicted == histograms).sum().item()
  return correct/total

!pip install bpy

image_val = image_test[:100]
histograms_val = histograms_test[:100]
image_test = image_test[100:]
histograms_test = histograms_test[100:]

print(len(image_val))
print(len(image_test))

dataset_train = HistogramDataset('train')
dataset_val = HistogramValDataset('test')
#dataset_test = HistogramDataset(image_test, histograms_test)
train_loader = DataLoader(dataset_train, batch_size=5, shuffle=True)
val_loader = DataLoader(dataset_val, batch_size=1)
#test_loader = DataLoader(dataset_test, batch_size=32)

# dataset_train = HistogramDataset(image_train, histograms_train)
# dataset_val = HistogramDataset(image_val, histograms_val)
# dataset_test = HistogramDataset(image_test, histograms_test)
# train_loader = DataLoader(dataset_train, batch_size=32, shuffle=True)
# val_loader = DataLoader(dataset_val, batch_size=32)
# test_loader = DataLoader(dataset_test, batch_size=32)

model = HistogramClassifier(num_classes=40) #len(histograms_train[0]))
device = torch.device('cuda:0') #if torch.cuda.is_available() else torch.device('cpu')
model = model.to(device)
#criterion = nn.CrossEntropyLoss()
criterion = nn.L1Loss(reduction='sum')
optimizer = optim.Adam(model.parameters(), lr=0.0001)

model = HistogramClassifier(num_classes=10) #len(histograms_train[0]))
device = torch.device('cuda:0') #if torch.cuda.is_available() else torch.device('cpu')
model = model.to(device)
#criterion = nn.CrossEntropyLoss()
criterion = nn.L1Loss(reduction='sum')
optimizer = optim.Adam(model.parameters(), lr=0.0001)

#Train


for epoch in range(5):
  train_loss = train(model, train_loader, criterion, optimizer)

  val_loss = validate(model, val_loader, criterion)

  print(f'Epoch {epoch+1}: train loss = {train_loss:.4f}, val loss={val_loss:.4f}')
  torch.save(model.state_dict(), '/content/drive/MyDrive/Dataset/model_method2.pth')
  torch.save(model, '/content/drive/MyDrive/Dataset/model_method2.pth')

device = torch.device('cuda:0') #if torch.cuda.is_available() else torch.device('cpu')
model = torch.load("/content/drive/MyDrive/Dataset/model_method2.pth")
model = model.to(device)
criterion = nn.L1Loss(reduction='sum')
optimizer = optim.Adam(model.parameters(), lr=0.0001)

for epoch in range(5):
  train_loss = train(model, train_loader, criterion, optimizer)

  val_loss = validate(model, val_loader, criterion)

  print(f'Epoch {epoch+1}: train loss = {train_loss:.4f}, val loss={val_loss:.4f}')
  torch.save(model.state_dict(), '/content/drive/MyDrive/Dataset/model_method22.pth')
  torch.save(model, '/content/drive/MyDrive/Dataset/model_method2.pth')

device = torch.device('cuda:0') #if torch.cuda.is_available() else torch.device('cpu')
model = torch.load("/content/drive/MyDrive/Dataset/model_method2.pth")
model = model.to(device)
criterion = nn.L1Loss(reduction='sum')
optimizer = optim.Adam(model.parameters(), lr=0.0001)

for epoch in range(3):
  train_loss = train(model, train_loader, criterion, optimizer)

  val_loss = validate(model, val_loader, criterion)

  print(f'Epoch {epoch+1}: train loss = {train_loss:.4f}, val loss={val_loss:.4f}')
  torch.save(model.state_dict(), '/content/drive/MyDrive/Dataset/model_method22.pth')
  torch.save(model, '/content/drive/MyDrive/Dataset/model_method2.pth')

#bins=10
for epoch in range(5):
  train_loss = train(model, train_loader, criterion, optimizer)

  val_loss = validate(model, val_loader, criterion)

  print(f'Epoch {epoch+1}: train loss = {train_loss:.4f}, val loss={val_loss:.4f}')
  torch.save(model.state_dict(), '/content/drive/MyDrive/Dataset/model_method3.pth')
  torch.save(model, '/content/drive/MyDrive/Dataset/model_method3.pth')

#Test
accuracy = test(model, val_loader)
print(f'Test accuracy = {accuracy:.4f}')

import torch
import numpy as np
import matplotlib.pyplot as plt
import json
import torchvision.transforms.functional as F


plt.rcParams["savefig.bbox"] = 'tight'


# def show(imgs):
#     if not isinstance(imgs, list):
#         imgs = [imgs]
#     fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)
#     for i, img in enumerate(imgs):
#         img = img.detach()
#         img = F.to_pil_image(img)
#         axs[0, i].imshow(np.asarray(img))
#         axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])

from torchvision.utils import draw_bounding_boxes
from torchvision.utils import make_grid
from torchvision.io import read_image
from torchvision.io import read_image, ImageReadMode


images_train_path = list(sorted(os.listdir("/content/drive/MyDrive/Dataset/renders/test")))
images_train_path = images_train_path[20:21]
all_diameters = []
coy = 1
for img_path1 in images_train_path:
    img_path = os.path.join("/content/drive/MyDrive/Dataset/renders/test", img_path1)
    ch = "."
    ch_index = img_path1.find(ch)
    annot = img_path1[:ch_index]
    annot_path = f"/content/drive/MyDrive/Dataset/particles/test/{annot}.json"
    print(annot_path)
    img = read_image(img_path, mode=ImageReadMode.RGB)
    #show(img)

    #annotations_file = open("/content/drive/MyDrive/Dataset/particles/test/particles8902.json")
    annotations_file = open(annot_path)
    annotations = json.load(annotations_file)
    img_filename = annotations['distribution']
    points = annotations['particles']
    size = []
    tn = []
    coy+=1

    diameters = []

    scale = np.array([11.0, 11.0, 2.0])
    import bpy
    import bpy_extras
    from mathutils import Vector

    #print(len(points))
    cout = 0
    i=6
    j=0
    siz=0
    for el in points:
      size.append(el['size'])

      x = (scale[0] * (el['x']) - scale[0]/2)
      y = (scale[1] * (el['y']) - scale[1]/2)
      z = (scale[2] * (el['z']) - scale[2]/2)
      if i == 6:
        scene = bpy.data.scenes["Scene"]

    # Set render resolution
        scene.render.resolution_x = 1024
        scene.render.resolution_y = 1024

    # Set camera rotation in euler angles
        scene.camera.rotation_mode = 'XYZ'
        scene.camera.rotation_euler = (0.0, 0.0, 0.0)

    # set the camera position
        scene.camera.location.x = 0
        scene.camera.location.y = 0
        scene.camera.location.z = 10

        bpy.context.scene.render.engine = 'CYCLES'
        bpy.ops.mesh.primitive_uv_sphere_add(
            radius=points[i]['size'],
            location=(x,y,z))
        i += 1

      coord_3d = Vector((x, y, z))
      coords_2d = bpy_extras.object_utils.world_to_camera_view(scene, scene.camera, coord_3d)

      x = (coords_2d.x) * (1024)
      y = (1 - coords_2d.y)* (1024)
      if x + (el['size']) *128 <= 30 or x - (el['size']) *128 >= 1000 or y + (el['size']) *128 <=30 or y - (el['size']) *128 >=1000:
        continue
      relsize=((el['size']) *128 + z)
      x1 = 0 if x - relsize < 0 else x - relsize
      y1 = 0 if y - relsize < 0 else y - relsize
      x2 = 1024 if x + relsize > 1024 else x + relsize
      y2 = 1024 if y + relsize > 1024 else y + relsize
      flag = False
      repeats = []
      repeats2 = []
      for el2 in tn:
        #print()
        xA = max(x1, el2[0])
        yA = max(y1, el2[1])
        xB = min(x2, el2[2])
        yB = min(y2, el2[3])

        interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)
        boxAArea = (x2 - x1 + 1) * (y2 - y1 + 1)
        boxBArea = (el2[2] - el2[0] + 1) * (el2[3] - el2[1] + 1)
        inter1 = (interArea/boxAArea)*100
        inter2 = (interArea/boxBArea)*100

        if inter1 > 50 and el2[4]>el['z']:
          flag = True
          break
        if inter2 > 50 and el2[4] < el['z']:
          repeats.append(el2)

      if not flag:
        for element in repeats:
          tn.remove(element)
        tn.append([x1, y1, x2, y2, el['z'], el['size']])
        #diameters.append(relsize*2)
      repeats.clear()

    num_objs = len(size)
    tn2 = [elem[0:4] for elem in tn ]
    diameters = [(elem[5]*128)*2 for elem in tn]
    all_diameters.append(diameters)

for diameters in all_diameters:
  #diam, bn, _ = plt.hist(diameters, bins=40)  # density=False would make counts
  hst3, bn3, _ = plt.hist(diameters, bins=40)

for diameters in all_diameters:
  #diam, bn, _ = plt.hist(diameters, bins=40)  # density=False would make counts
  hst3, bn3, _ = plt.hist(diameters, bins=40)

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model = torch.load("/content/drive/MyDrive/Dataset/model_method2.pth")
#model = torch.load("/content/drive/MyDrive/Dataset/model_method3.pth", map_location=torch.device('cpu'))
model.eval()

import matplotlib.pyplot as plt
import os
from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks
from torchvision.io import read_image, ImageReadMode
from torchvision.ops.boxes import masks_to_boxes
from torchvision import tv_tensors
import gc


images_train_path = list(sorted(os.listdir("/content/drive/MyDrive/Dataset/renders/test")))
images_train_path = images_train_path[20:21]


all_pred_boxes = []
for img in images_train_path:
  img_path = os.path.join("/content/drive/MyDrive/Dataset/renders/test", img)
  print(img_path)
  image = read_image(img_path, mode=ImageReadMode.RGB).float()
  img= tv_tensors.Image(image)
  #eval_transform = get_transform(train=False)

  model.eval()
  img = img.to(torch.device('cuda:0'))
  outputs = model(img.unsqueeze(0))
  all_pred_boxes.append(outputs)
  del outputs
  gc.collect()
  torch.cuda.empty_cache()
  # with torch.no_grad():
  #     #x = eval_transform(image)
  #     # convert RGBA -> RGB and move to device
  #     img = img.to(device)
  #     predictions = model(img)
  #     print(predictions)
      #pred = predictions[0]


  #image = (255.0 * (image - image.min()) / (image.max() - image.min())).to(torch.uint8)
  #image = image[:3, ...]
  #pred_labels = [f"pedestrian: {score:.3f}" for label, score in zip(pred["labels"], pred["scores"])]
  # pred_boxes = pred["boxes"].long()
  # pred_boxes_cpu=pred_boxes.cpu()
  # pred_max = []
  # for el in pred_boxes_cpu:
  #   pred_max.append(max(el[2]-el[0], el[3]-el[1]))
  # all_pred_boxes.append(pred_max)

import matplotlib.pyplot as plt
import numpy as np
# np.random.seed(42)
# x = np.random.normal(size=1000)

random_val = []
random_val2 = []
for diameters in all_diameters:
  #diam, bn, _ = plt.hist(diameters, bins=40)  # density=False would make counts
  (hst, bin_edges) = np.histogram(diameters, bins=40)
  #plt.ylabel('Probability')
  #plt.xlabel('Data');
  div = hst/sum(hst)
  random_values = np.random.choice(bin_edges[:-1], size=500, p=(div/sum(div)))
  random_val.append(random_values)
  #print(random_values)

outputs_cpu = []
for i in range(len(all_pred_boxes)):
  outputs_cpu.append(all_pred_boxes[i].cpu().detach().numpy())
for pred_max in range(len(outputs_cpu)):
  for j in range(len(outputs_cpu[pred_max][0])):
    if outputs_cpu[pred_max][0][j] < 0:
      outputs_cpu[pred_max][0][j] = 0
  print(outputs_cpu)
  #(hst2, bin_edges2) = np.histogram(outputs_cpu[pred_max][0], bins=40)
  random_values2 = np.random.choice(bin_edges[:-1], size=176, p=(outputs_cpu[pred_max][0])/sum(outputs_cpu[pred_max][0]))
  random_val2.append(random_values2)

for diameters in all_diameters:
  #diam, bn, _ = plt.hist(diameters, bins=40)  # density=False would make counts
  hst3, bn3, _ = plt.hist(diameters, bins=40)

for pred_max in range(len(outputs_cpu)):
  #print(outputs_cpu)
  #(hst2, bin_edges2) = np.histogram(outputs_cpu[pred_max][0], bins=40)
  print(sum(outputs_cpu[pred_max][0]))
  random_values2 = np.random.choice(bin_edges[:-1], size=204, p=(outputs_cpu[pred_max][0]))#/sum(outputs_cpu[pred_max][0]))
  random_val2.append(random_values2)
hst3, bn3, _ = plt.hist(random_values2, bins=40)

norm_hst = hst/sum(hst)

norm_hst2 = []
for i in range(len(outputs)):
  norm_hst2.append(outputs[i])

outputs_cpu = []
for i in range(len(all_pred_boxes)):
  outputs_cpu.append(all_pred_boxes[i].cpu().detach().numpy())

norm_hs2 = outputs_cpu.detach().numpy()

import math

kl_div1 = 0
kl_div2 = 0
count1 = 0
count2 = 0
for i in range(len(random_val)):
  # # Ensuring all values are positive
  hist2 = np.abs(random_val2[i]).astype('float64')
  # #print(hist2)
  diameters2 = np.abs(random_val[i]).astype('float64')
  # #print(diameters2)
  # # Normalize to form a valid probability distribution
  hist2 /= np.sum(hist2)
  diameters2 /= np.sum(diameters2)
  # #print(hist2)
  # #print(diameters2)
  #norm_hst = random_val[i]
  #print(norm_hst)
  #for j in range(len(outputs_cpu[i][0])):
  #  if outputs_cpu[i][0][j] < 0:
   #   outputs_cpu[i][0][j] = 0
  #print(outputs_cpu[i][0])
  # # Now, Calculate the KL Divergence
  kl_divergence = np.sum(diameters2 * np.log(diameters2 / hist2))
  if not math.isnan(kl_divergence):
    kl_div1 += kl_divergence
    count1+=1
  print(kl_divergence, "first")
  #print("KL Divergence:", kl_divergence)


  # Ensuring all values are positive
  hist2 = np.abs(random_val2[i]).astype('float64')
  # #print(hist2)
  diameters2 = np.abs(random_val[i]).astype('float64')
  # #print(diameters2)
  # # Normalize to form a valid probability distribution
  hist2 /= np.sum(hist2)
  diameters2 /= np.sum(diameters2)
  # #print(hist2)
  # #print(diameters2)
  # # Now, Calculate the KL Divergence
  kl_divergence = np.sum(hist2 * np.log(hist2 / diameters2))
  if not math.isnan(kl_divergence):
    kl_div2 += kl_divergence
    count2+=1
  print(kl_divergence, "second")
  #print("KL Divergence:", kl_divergence)

print(kl_div1/count1)
print(kl_div2/count2)

kl_div1 = 0
kl_div2 = 0
count1 = 0
count2 = 0
for i in range(len(random_val)):
  # # Ensuring all values are positive
  # hist2 = np.abs(random_val2[i]).astype('float64')
  # #print(hist2)
  # diameters2 = np.abs(random_val[i]).astype('float64')
  # #print(diameters2)
  # # Normalize to form a valid probability distribution
  # hist2 /= np.sum(hist2)
  # diameters2 /= np.sum(diameters2)
  # #print(hist2)
  # #print(diameters2)
  norm_hst = random_val[i]
  print(norm_hst)
  for j in range(len(outputs_cpu[i][0])):
    if outputs_cpu[i][0][j] < 0:
      outputs_cpu[i][0][j] = 0
  print(outputs_cpu[i][0])
  # # Now, Calculate the KL Divergence
  kl_divergence = np.sum(norm_hst * np.log(norm_hst / outputs_cpu[i][0]))
  if not math.isnan(kl_divergence):
    kl_div1 += kl_divergence
    count1+=1
  print(kl_divergence, "first")
  #print("KL Divergence:", kl_divergence)


  # Ensuring all values are positive
  # hist2 = np.abs(random_val2[i]).astype('float64')
  # #print(hist2)
  # diameters2 = np.abs(random_val[i]).astype('float64')
  # #print(diameters2)
  # # Normalize to form a valid probability distribution
  # hist2 /= np.sum(hist2)
  # diameters2 /= np.sum(diameters2)
  # #print(hist2)
  # #print(diameters2)
  # # Now, Calculate the KL Divergence
  kl_divergence = np.sum(outputs_cpu[i][0] * np.log(outputs_cpu[i][0] / norm_hst))
  if not math.isnan(kl_divergence):
    kl_div2 += kl_divergence
    count2+=1
  print(kl_divergence, "second")
  #print("KL Divergence:", kl_divergence)

print(kl_div1/count1)
print(kl_div2/count2)